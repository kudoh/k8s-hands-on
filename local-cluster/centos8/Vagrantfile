# -*- mode: ruby -*-
# vi: set ft=ruby :

####################################################################
# this config is not working... k8s not yet support centos8
####################################################################

k8s_version = "1.17"

Vagrant.configure("2") do |config|

  $common_setup_script = <<-SCRIPT

  # SWAP無効化(kubelet要件)
  swapoff -a
  sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab
  
  # SELinux無効化(kubelet要件)
  setenforce 0
  sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

  # yumパッケージ最新化
  dnf update -y

  # yumリポジトリにkubernetesリポジトリを追加
  cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

  VERSION=$(dnf list kubeadm --showduplicates --disableexcludes=kubernetes -y | sort -r | grep #{k8s_version} | head -1 | awk '{print $2}')
  dnf install -y kubelet-$VERSION kubeadm-$VERSION kubectl-$VERSION --disableexcludes=kubernetes
  systemctl enable kubelet && systemctl start kubelet

  # Containerd
  # https://kubernetes.io/docs/setup/cri/#containerd
  cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
  modprobe overlay
  modprobe br_netfilter

  # Setup required sysctl params, these persist across reboots.
  cat > /etc/sysctl.d/99-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

  # Install containerd
  ## Set up the repository
  ### Install required packages
  dnf install yum-utils device-mapper-persistent-data lvm2 -y
  
  ### Add docker repository
  yum-config-manager \
      --add-repo \
      https://download.docker.com/linux/centos/docker-ce.repo
  
  ## Install containerd
  dnf install containerd.io -y
  
  # Configure containerd
  mkdir -p /etc/containerd
  containerd config default > /etc/containerd/config.toml
  
  # Restart containerd
  systemctl restart containerd
  
  # Network Pluginの要件  
  # https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements
  # sysctl net.bridge.bridge-nf-call-iptables=1
  # sysctl net.bridge.bridge-nf-call-ip6tables=1

  sysctl --system

  IPADDR=$(ip a show eth1 | grep inet | grep -v inet6 | awk '{print $2}' | cut -f1 -d/)
  # bind private ip to kubelet
  sed -i "/KUBELET_EXTRA_ARGS=/c\KUBELET_EXTRA_ARGS=--node-ip=$IPADDR" /etc/sysconfig/kubelet
  # restart kubelet
  systemctl daemon-reload
  systemctl restart kubelet
  
  # Install iSCSI for OpenEBS
  dnf install iscsi-initiator-utils -y
  systemctl enable iscsid && systemctl start iscsid

  SCRIPT

  $master_node_script = <<-SCRIPT

  IPADDR=$(ip a show eth1 | grep inet | grep -v inet6 | awk '{print $2}' | cut -f1 -d/)
  HOSTNAME=$(hostname -s)
  K8S_VERSION=$(kubelet --version | cut -f2 -d ' ')
  # Control Plane初期化(continerd)
  # 使用するCRI/CNIによってパラメータ調整が必要. FlannelはPod CIDRが`10.244.0.0/16`である必要がある
  kubeadm init --apiserver-advertise-address=$IPADDR --apiserver-cert-extra-sans=$IPADDR --kubernetes-version=$K8S_VERSION \
    --node-name $HOSTNAME --pod-network-cidr=10.244.0.0/16 --cri-socket=/run/containerd/containerd.sock

  # CNI-flannel
  # https://github.com/coreos/flannel
  # https://github.com/coreos/flannel/blob/master/Documentation/troubleshooting.md#vagrant
  # https://medium.com/@anilkreddyr/kubernetes-with-flannel-understanding-the-networking-part-1-7e1fe51820e4
  export KUBECONFIG=/etc/kubernetes/admin.conf
  kubectl apply -f https://raw.githubusercontent.com/kudoh/k8s-hands-on/master/local-cluster/kube-flannel-0.11.yml
  
  # Control PlaneをDaemonSetとして実行する(SelfHosting)
  # これをやらないとAPI Server からCoreDNSにアクセスできずAdmission WebHookが機能しなかった
  # kubeadmはデフォルトでStatic PodとしてControl Planeを起動していて、これだとPod内の/etc/resolv.confがOS側と同じになる
  kubeadm alpha self-hosting pivot -f

  # Control Plane -> Service Net(10.96.0.0/12)への経路を追加
  ip route add 10.96.0.0/12 via 10.244.0.1 dev cni0

  # kubeadm join スクリプト作成
  SHARED_DIR=/home/vagrant/shared
  mkdir -p $SHARED_DIR
  kubeadm token create --print-join-command > $SHARED_DIR/kubeadm_join_cmd.sh
  chmod +x $SHARED_DIR/kubeadm_join_cmd.sh
  
  # generating client certificate
  openssl req -new -newkey rsa:2048 -nodes \
    -keyout $SHARED_DIR/k8s-admin.key \
    -out k8s-admin.csr \
    -subj "/CN=admin/O=mamezou-sre"
  chmod 600 $SHARED_DIR/k8s-admin.key
  MASTER_PKI=/etc/kubernetes/pki
  openssl x509 -req -days 60 \
    -in k8s-admin.csr \
    -CA $MASTER_PKI/ca.crt \
    -CAkey $MASTER_PKI/ca.key \
    -CAcreateserial \
    -out $SHARED_DIR/k8s-admin.crt -sha256
  kubectl create clusterrolebinding --clusterrole=cluster-admin --user admin local-admin

  rm -f $SHARED_DIR/ca.crt
  cp -f $MASTER_PKI/ca.crt $SHARED_DIR/ca.crt
  
  SCRIPT

  $worker_node_script = <<-SCRIPT

  echo "worker node settings..."

  sh /home/vagrant/shared/kubeadm_join_cmd.sh

  SCRIPT
  
  #config.vm.provision "shell", inline: $common_setup_script

  # Master Node  
  (1..1).each do |i|
    config.vm.define "k8s-master#{i}" do |master|
      master.vm.box = "centos/8"
      config.vm.box_version = "1905.1"
      master.vm.hostname = "k8s-master#{i}"
      master.vm.synced_folder "shared/", "/home/vagrant/shared", type: "virtualbox", create: true, group: "vagrant", owner: "vagrant"

      master.vm.network "private_network", ip: "172.16.10.#{i + 10}"
      master.vm.provider "virtualbox" do |vb|
        vb.memory = 2048
        vb.cpus = 2
      end
      # vagrant plugin install vagrant-disksize
      master.disksize.size = '20GB'
      # 全部手動でやる場合は以下をコメントアウト
      # master.vm.provision "shell", inline: $master_node_script
    end
  end

  # Worker Node  
  (1..3).each do |i|
    config.vm.define "k8s-worker#{i}" do |worker|
      worker.vm.box = "centos/8"
      config.vm.box_version = "1905.1"
      worker.vm.hostname = "k8s-worker#{i}"
      worker.vm.synced_folder "shared/", "/home/vagrant/shared", type: "virtualbox", create: true, group: "vagrant", owner: "vagrant"
      worker.vm.network "private_network", ip: "172.16.20.#{i + 10}"
      worker.vm.provider "virtualbox" do |vb|
        vb.memory = 4096
        vb.cpus = 1
      end
      # vagrant plugin install vagrant-disksize
      worker.disksize.size = '50GB'
      # 全部手動でやる場合は以下をコメントアウト
      # worker.vm.provision "shell", inline: $worker_node_script
    end
  end
end
