# -*- mode: ruby -*-
# vi: set ft=ruby :

k8s_version = "1.17"

Vagrant.configure("2") do |config|

  $common_setup_script = <<-SCRIPT

  # SWAP無効化(kubelet要件)
  swapoff -a
  sed -i '/ swap / s/^/#/' /etc/fstab
  
  apt-get update && apt-get install -y apt-transport-https curl
  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
  cat <<EOF | tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

  apt-get update
  VERSION=$(apt-cache show kubelet | grep "Version: ${k8s_version}" | head -1 | cut -d' ' -f 2)
  apt-get install -y kubelet=${VERSION} kubeadm=${VERSION} kubectl=${VERSION}
  
  apt-mark hold kubelet kubeadm kubectl

  # https://kubernetes.io/docs/setup/cri/#containerd
  cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
  modprobe overlay
  modprobe br_netfilter

  # Setup required sysctl params, these persist across reboots.
  cat > /etc/sysctl.d/99-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
  sysctl --system

  # Install containerd
  ## Set up the repository
  ### Install packages to allow apt to use a repository over HTTPS
  apt-get update && apt-get install -y apt-transport-https ca-certificates curl software-properties-common

  ### Add Docker’s official GPG key
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

  ### Add Docker apt repository.
  add-apt-repository \
      "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
      $(lsb_release -cs) \
      stable"

  ## Install containerd
  apt-get update && apt-get install -y containerd.io

  # Configure containerd
  mkdir -p /etc/containerd
  containerd config default > /etc/containerd/config.toml

  # Restart containerd
  systemctl restart containerd

  # bind private ip to kubelet
  #sed -i "/KUBELET_EXTRA_ARGS=/c\KUBELET_EXTRA_ARGS=--node-ip=$IPADDR" /etc/default/kubelet
  IPADDR=$(ip a show eth1 | grep inet | grep -v inet6 | awk '{print $2}' | cut -f1 -d/)
  echo "KUBELET_EXTRA_ARGS=--node-ip=$IPADDR" >> /etc/default/kubelet
  #echo "KUBELET_EXTRA_ARGS=--cgroup-driver=systemd" >> /etc/default/kubelet
  # restart kubelet
  #systemctl daemon-reload
  #systemctl restart kubelet
  
  # Install iSCSI for OpenEBS
  apt-get update && apt-get install open-iscsi
  systemctl enable iscsid && sudo systemctl start iscsid

  SCRIPT

  $master_node_script = <<-SCRIPT

  IPADDR=$(ip a show eth1 | grep inet | grep -v inet6 | awk '{print $2}' | cut -f1 -d/)
  HOSTNAME=$(hostname -s)
  K8S_VERSION=$(kubelet --version | cut -f2 -d ' ')
  # Control Plane初期化(continerd)
  # 使用するCRI/CNIによってパラメータ調整が必要. FlannelはPod CIDRが`10.244.0.0/16`である必要がある
  kubeadm init --apiserver-advertise-address=$IPADDR --apiserver-cert-extra-sans=$IPADDR --kubernetes-version=$K8S_VERSION \
    --node-name $HOSTNAME --pod-network-cidr=10.244.0.0/16 --cri-socket=unix:///run/containerd/containerd.sock

  # CNI-flannel
  # https://github.com/coreos/flannel
  # https://github.com/coreos/flannel/blob/master/Documentation/troubleshooting.md#vagrant
  # https://medium.com/@anilkreddyr/kubernetes-with-flannel-understanding-the-networking-part-1-7e1fe51820e4
  export KUBECONFIG=/etc/kubernetes/admin.conf
  kubectl apply -f https://raw.githubusercontent.com/kudoh/k8s-hands-on/master/local-cluster/kube-flannel-0.11.yml
  
  # Control PlaneをDaemonSetとして実行する(SelfHosting)
  # これをやらないとAPI Server からCoreDNSにアクセスできずAdmission WebHookが機能しなかった
  # kubeadmはデフォルトでStatic PodとしてControl Planeを起動していて、これだとPod内の/etc/resolv.confがOS側と同じになる
  #kubeadm alpha self-hosting pivot -f

  # Control Plane -> Service Net(10.96.0.0/12)への経路を追加
  #ip route add 10.96.0.0/12 via 10.244.0.1 dev cni0

  # kubeadm join スクリプト作成
  SHARED_DIR=/home/vagrant/shared
  mkdir -p $SHARED_DIR
  kubeadm token create --print-join-command > $SHARED_DIR/kubeadm_join_cmd.sh
  chmod +x $SHARED_DIR/kubeadm_join_cmd.sh
  
  # generating client certificate
  openssl req -new -newkey rsa:2048 -nodes \
    -keyout $SHARED_DIR/k8s-admin.key \
    -out k8s-admin.csr \
    -subj "/CN=admin/O=mamezou-sre"
  chmod 600 $SHARED_DIR/k8s-admin.key
  MASTER_PKI=/etc/kubernetes/pki
  openssl x509 -req -days 360 \
    -in k8s-admin.csr \
    -CA $MASTER_PKI/ca.crt \
    -CAkey $MASTER_PKI/ca.key \
    -CAcreateserial \
    -out $SHARED_DIR/k8s-admin.crt -sha256
  kubectl create clusterrolebinding --clusterrole=cluster-admin --user admin local-admin

  rm -f $SHARED_DIR/ca.crt
  cp -f $MASTER_PKI/ca.crt $SHARED_DIR/ca.crt
  
  SCRIPT

  $worker_node_script = <<-SCRIPT

  echo "worker node settings..."

  sh /home/vagrant/shared/kubeadm_join_cmd.sh

  SCRIPT
  
  config.vm.provision "shell", inline: $common_setup_script

  # Master Node  
  (1..1).each do |i|
    config.vm.define "k8s-master#{i}" do |master|
      master.vm.box = "bento/ubuntu-18.04"
      config.vm.box_version = "201912.14.0"
      master.vm.hostname = "k8s-master#{i}"
      master.vm.synced_folder "shared/", "/home/vagrant/shared", type: "virtualbox", create: true, group: "vagrant", owner: "vagrant"

      master.vm.network "private_network", ip: "172.16.10.#{i + 10}"
      master.vm.provider "virtualbox" do |vb|
        vb.memory = 2048
        vb.cpus = 2
      end
      # vagrant plugin install vagrant-disksize
      #master.disksize.size = '20GB'
      # 全部手動でやる場合は以下をコメントアウト
      master.vm.provision "shell", inline: $master_node_script
    end
  end

  # Worker Node  
  (1..3).each do |i|
    config.vm.define "k8s-worker#{i}" do |worker|
      worker.vm.box = "bento/ubuntu-18.04"
      config.vm.box_version = "201912.14.0"
      worker.vm.hostname = "k8s-worker#{i}"
      worker.vm.synced_folder "shared/", "/home/vagrant/shared", type: "virtualbox", create: true, group: "vagrant", owner: "vagrant"
      worker.vm.network "private_network", ip: "172.16.20.#{i + 10}"
      worker.vm.provider "virtualbox" do |vb|
        vb.memory = 4096
        vb.cpus = 1
      end
      # vagrant plugin install vagrant-disksize
      #worker.disksize.size = '50GB'
      # 全部手動でやる場合は以下をコメントアウト
      worker.vm.provision "shell", inline: $worker_node_script
    end
  end
end
